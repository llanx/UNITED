---
phase: 08-voice-channels
plan: 02
type: execute
wave: 2
depends_on:
  - "08-01"
files_modified:
  - client/src/renderer/src/voice/VoiceManager.ts
  - client/src/renderer/src/voice/AudioPipeline.ts
  - client/src/renderer/src/voice/SignalingClient.ts
  - client/src/renderer/src/stores/voice.ts
  - client/src/renderer/src/stores/index.ts
  - client/src/renderer/src/hooks/useVoice.ts
  - client/src/main/ipc/voice.ts
  - client/src/main/voice/ptt.ts
  - client/src/main/ipc/channels.ts
  - client/src/preload/index.ts
  - client/src/main/ws/voice-events.ts
  - client/package.json
  - shared/ipc-bridge.d.ts
autonomous: true
requirements:
  - VOICE-01
  - VOICE-02
  - VOICE-03
  - VOICE-04

must_haves:
  truths:
    - "Client can join a voice channel and establish WebRTC peer connections with existing participants"
    - "Client can mute microphone and deafen all incoming audio with immediate local effect"
    - "Client detects speaking state via AnalyserNode RMS and broadcasts to other participants"
    - "Client supports push-to-talk via uiohook-napi global hotkey with keydown/keyup events"
    - "Per-user volume control adjusts GainNode for individual remote streams (0-200%)"
    - "Connection quality metrics polled from getStats every 2 seconds"
  artifacts:
    - path: "client/src/renderer/src/voice/VoiceManager.ts"
      provides: "Full mesh WebRTC connection lifecycle"
      contains: "RTCPeerConnection"
    - path: "client/src/renderer/src/voice/AudioPipeline.ts"
      provides: "Web Audio API routing for VAD, volume, deafen"
      contains: "AnalyserNode"
    - path: "client/src/renderer/src/stores/voice.ts"
      provides: "Voice state slice (participants, mute, deafen, speaking)"
      contains: "VoiceSlice"
    - path: "client/src/main/voice/ptt.ts"
      provides: "Global keyboard hook for push-to-talk"
      contains: "uiohook-napi"
    - path: "client/src/main/ipc/voice.ts"
      provides: "Voice IPC handlers"
      contains: "ipcMain.handle"
  key_links:
    - from: "client/src/renderer/src/voice/VoiceManager.ts"
      to: "client/src/renderer/src/voice/SignalingClient.ts"
      via: "SDP/ICE exchange via WS"
      pattern: "sendSignaling"
    - from: "client/src/renderer/src/voice/VoiceManager.ts"
      to: "client/src/renderer/src/voice/AudioPipeline.ts"
      via: "Remote stream routing through Web Audio"
      pattern: "addRemoteStream"
    - from: "client/src/renderer/src/voice/SignalingClient.ts"
      to: "client/src/main/ws/voice-events.ts"
      via: "WS voice event forwarding from main to renderer"
      pattern: "PUSH_VOICE_EVENT"
    - from: "client/src/main/voice/ptt.ts"
      to: "client/src/renderer/src/stores/voice.ts"
      via: "IPC push of PTT key state"
      pattern: "PUSH_PTT_STATE"
---

<objective>
Build the client-side voice engine: WebRTC full-mesh connection management, Web Audio API pipeline for VAD/volume/deafen, WS voice event handling, push-to-talk via uiohook-napi, Zustand voice state, and IPC bridge.

Purpose: Implements the core voice communication stack that the UI will consume. Separates the complex WebRTC/audio machinery from the visual components so each can be built and tested independently.

Output: VoiceManager (WebRTC), AudioPipeline (Web Audio), SignalingClient (WS), PTT module, VoiceSlice store, IPC handlers, preload bridge.
</objective>

<execution_context>
@C:/Users/matts/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/matts/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-voice-channels/08-CONTEXT.md
@.planning/phases/08-voice-channels/08-RESEARCH.md
@.planning/phases/08-voice-channels/08-01-SUMMARY.md

@client/src/main/ipc/channels.ts
@client/src/preload/index.ts
@client/src/renderer/src/stores/index.ts
@shared/proto/voice.proto

<interfaces>
<!-- Key types and contracts from 08-01 and existing codebase -->

From shared/proto/voice.proto (created in 08-01):
- VoiceJoinRequest, VoiceJoinResponse (with participants + ice_servers)
- VoiceLeaveRequest, VoiceLeaveEvent
- VoiceSdpOffer, VoiceSdpAnswer, VoiceIceCandidate
- VoiceStateUpdate, VoiceSpeakingEvent
- VoiceParticipant (user_id, display_name, pubkey, muted, deafened)
- VoiceParticipantJoinedEvent
- IceServer (urls, username, credential)

WS Envelope voice fields: 180-189

From client/src/main/ipc/channels.ts:
- IPC constant pattern: `IPC = { ... } as const`
- Push event pattern: `PUSH_*` constants for main -> renderer events

From client/src/preload/index.ts:
- API exposed via `contextBridge.exposeInMainWorld('united', { ... })`
- Namespaced: `api.channels.*`, `api.dm.*`, etc.

From client/src/renderer/src/stores/index.ts:
- Zustand slice pattern: `StateCreator<RootStore, [], [], SliceType>`
- Combined via spread in root store creator
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Voice engine (VoiceManager, AudioPipeline, SignalingClient) and WS event handling</name>
  <files>
    client/src/renderer/src/voice/VoiceManager.ts
    client/src/renderer/src/voice/AudioPipeline.ts
    client/src/renderer/src/voice/SignalingClient.ts
    client/src/main/ws/voice-events.ts
  </files>
  <action>
    **1. Create `client/src/renderer/src/voice/SignalingClient.ts`:**
    - Class that sends/receives voice signaling messages via the existing WS connection.
    - `sendJoin(channelId: string): void` — sends VoiceJoinRequest via IPC
    - `sendLeave(channelId: string): void` — sends VoiceLeaveRequest via IPC
    - `sendSdpOffer(targetUserId: string, sdp: string, channelId: string): void`
    - `sendSdpAnswer(targetUserId: string, sdp: string, channelId: string): void`
    - `sendIceCandidate(targetUserId: string, candidateJson: string, channelId: string): void`
    - `sendStateUpdate(channelId: string, muted: boolean, deafened: boolean): void`
    - `sendSpeaking(channelId: string, speaking: boolean): void`
    - Event callbacks: `onJoinResponse`, `onParticipantJoined`, `onParticipantLeft`, `onSdpOffer`, `onSdpAnswer`, `onIceCandidate`, `onStateUpdate`, `onSpeaking`
    - All send methods call `window.united.voice.*` (IPC to main, which forwards to WS)
    - All receive callbacks are registered via `window.united.onVoiceEvent(callback)` (push from main)
    - `dispose()` method to remove all listeners

    **2. Create `client/src/renderer/src/voice/AudioPipeline.ts`:**
    - Manages AudioContext and all audio routing
    - `init()`: Create AudioContext (must be called on user gesture -- the voice channel join click). Call `audioContext.resume()` to handle autoplay policy (Pitfall 1).
    - `captureLocalMic(constraints?)`: Call `navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true } })`. On macOS, check mic permission first via IPC call to `systemPreferences.getMediaAccessStatus('microphone')` (Pitfall 2). Return the MediaStream.
    - `addRemoteStream(peerId: string, stream: MediaStream)`: Create MediaStreamSource -> GainNode -> AnalyserNode -> masterGain -> destination. Store GainNode and AnalyserNode per peer.
    - `removeRemoteStream(peerId: string)`: Disconnect and clean up audio nodes for this peer.
    - `setUserVolume(peerId: string, volume: number)`: Set GainNode value (volume / 100, supports 0-200).
    - `setMasterVolume(volume: number)`: Adjust masterGain.
    - `deafen(deafened: boolean)`: Set masterGain to 0 (deafened) or 1 (not).
    - `muteLocalMic(muted: boolean)`: Toggle `track.enabled` on all local audio tracks.
    - `isSpeaking(peerId: string, threshold: number): boolean`: Use AnalyserNode.getFloatTimeDomainData() to compute RMS. Return `rms > threshold`.
    - `getLocalRMS(threshold: number): boolean`: Same for local mic stream (for local VAD and settings indicator).
    - `setOutputDevice(deviceId: string)`: Use `audioContext.setSinkId(deviceId)` if supported, otherwise create `<audio>` elements with `setSinkId()`.
    - `dispose()`: Stop all tracks, close all audio nodes, close AudioContext. CRITICAL: must call `track.stop()` for ALL tracks, `pc.close()` is not enough (Pitfall 6).
    - Store local stream, analyser for local mic, and per-peer data in Maps.

    **3. Create `client/src/renderer/src/voice/VoiceManager.ts`:**
    - Core WebRTC full-mesh manager. One instance per voice session.
    - Constructor takes `SignalingClient` and `AudioPipeline` as dependencies.
    - `joinChannel(channelId: string, existingParticipants: VoiceParticipant[], iceServers: IceServer[])`:
      - Store iceServers as RTCConfiguration.
      - Capture local mic via AudioPipeline.captureLocalMic().
      - For each existing participant, determine offer/answer role: the peer with the lexicographically smaller user_id sends the offer (Pitfall 4 -- prevents duplicate connections).
      - Create RTCPeerConnection for each peer, add local audio track, set up event handlers.
    - `handleNewParticipant(participant: VoiceParticipant)`: Create peer connection when a new user joins. Apply same lexicographic offer rule.
    - `handleParticipantLeft(userId: string)`: Close and clean up that peer's RTCPeerConnection and audio routing.
    - `handleSdpOffer(fromUserId: string, sdp: string)`: Set remote description, create answer, send answer. Process queued ICE candidates (Pitfall 3).
    - `handleSdpAnswer(fromUserId: string, sdp: string)`: Set remote description. Process queued ICE candidates.
    - `handleIceCandidate(fromUserId: string, candidateJson: string)`: If remote description set, add candidate. Otherwise queue it (Pitfall 3 -- ICE candidate race condition).
    - `leaveChannel()`: Close all peer connections, stop all tracks, clean up AudioPipeline. Send VoiceLeaveRequest.
    - ICE candidate queueing: maintain `pendingCandidates: Map<string, RTCIceCandidateInit[]>` per peer. Process queue after `setRemoteDescription` succeeds.
    - `ontrack` handler: Route remote stream through AudioPipeline.addRemoteStream().
    - `onicecandidate` handler: Send via SignalingClient.sendIceCandidate().
    - `oniceconnectionstatechange` handler: If `failed`, call `pc.restartIce()` and create new offer. If `disconnected`, start 15s timer, if not recovered, remove peer (per CONTEXT.md).
    - `getStats(peerId: string)`: Call `pc.getStats()`, extract roundTripTime, fractionLost, jitter from `remote-inbound-rtp` reports with `kind === 'audio'`. Return quality classification: good/degraded/poor based on thresholds (rtt > 0.3s or loss > 5% = poor, rtt > 0.15s or loss > 2% = degraded).
    - Speaking detection loop: 50ms interval via requestAnimationFrame or setInterval, check local and all remote peers via AudioPipeline.isSpeaking(). When state changes, update store and send VoiceSpeakingEvent for local user.
    - Stats polling loop: 2s interval, poll getStats for each peer, update store with quality metrics.
    - Set Opus max bitrate to 40kbps: After connection established, call `sender.getParameters()` / `sender.setParameters()` with `encodings[0].maxBitrate = 40000`.

    **4. Create `client/src/main/ws/voice-events.ts`:**
    - Follow the exact pattern of `client/src/main/ws/chat-events.ts` and `client/src/main/ws/dm-events.ts`.
    - Decode protobuf Envelope from incoming WS binary messages.
    - Switch on payload.case for voice-related events.
    - Forward to renderer via `mainWindow.webContents.send(IPC.PUSH_VOICE_EVENT, { type, data })`.
    - Handle: voice_join_response, voice_leave_event, voice_sdp_offer, voice_sdp_answer, voice_ice_candidate, voice_state_update, voice_speaking_event, voice_participant_joined_event.
    - Import from the generated protobuf types (`@shared/gen/...` or however the project generates them — check existing pattern in chat-events.ts).
  </action>
  <verify>
    cd /c/Users/matts/united/client && npx tsc --noEmit 2>&1 | tail -10
  </verify>
  <done>
    VoiceManager handles full-mesh WebRTC lifecycle with ICE candidate queueing, lexicographic offer/answer role, Opus 40kbps, and auto-reconnect. AudioPipeline routes all audio through Web Audio API with per-user GainNode, AnalyserNode for VAD, and master gain for deafen. SignalingClient wraps IPC calls for all voice WS messages. voice-events.ts forwards WS voice events from main to renderer. TypeScript compiles without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Voice IPC, PTT, Zustand store, hooks, and preload bridge</name>
  <files>
    client/src/main/ipc/voice.ts
    client/src/main/voice/ptt.ts
    client/src/renderer/src/stores/voice.ts
    client/src/renderer/src/stores/index.ts
    client/src/renderer/src/hooks/useVoice.ts
    client/src/main/ipc/channels.ts
    client/src/preload/index.ts
    client/package.json
    shared/ipc-bridge.d.ts
  </files>
  <action>
    **1. Install uiohook-napi:**
    ```bash
    cd client && npm install uiohook-napi
    ```
    If Electron rebuild is needed (likely for native N-API module), add to the existing rebuild pipeline. Check how sodium-native and better-sqlite3 are rebuilt.

    **2. Create `client/src/main/voice/ptt.ts`:**
    - Import `uIOhook, UiohookKey` from `uiohook-napi`.
    - `DEFAULT_PTT_KEY = UiohookKey.Backquote` (backtick/grave key per research recommendation — most common PTT default, top-left of keyboard, rarely used in typing).
    - `startPTT(key: number, onActivate: () => void, onDeactivate: () => void)`: Register keydown/keyup handlers. Track `pttActive` flag to prevent repeated keydown events. Call `uIOhook.start()`.
    - `stopPTT()`: Call `uIOhook.stop()`. Remove handlers.
    - `changePTTKey(newKey: number)`: Stop, re-register with new key, start.
    - Export `DEFAULT_PTT_KEY` for settings UI.
    - PTT activation/deactivation sends IPC push to renderer: `mainWindow.webContents.send(IPC.PUSH_PTT_STATE, active)`.

    **3. Create `client/src/main/ipc/voice.ts`:**
    IPC handlers following the existing pattern (see `client/src/main/ipc/chat.ts`):
    - `IPC.VOICE_JOIN`: Send VoiceJoinRequest via WS. Start PTT if voice mode is PTT.
    - `IPC.VOICE_LEAVE`: Send VoiceLeaveRequest via WS. Stop PTT.
    - `IPC.VOICE_SEND_SDP_OFFER`: Forward SDP offer to WS.
    - `IPC.VOICE_SEND_SDP_ANSWER`: Forward SDP answer to WS.
    - `IPC.VOICE_SEND_ICE_CANDIDATE`: Forward ICE candidate to WS.
    - `IPC.VOICE_SEND_STATE_UPDATE`: Forward mute/deafen state to WS.
    - `IPC.VOICE_SEND_SPEAKING`: Forward speaking state to WS.
    - `IPC.VOICE_GET_DEVICES`: Return `navigator.mediaDevices.enumerateDevices()` (main process — actually this should be done in renderer since main process doesn't have getUserMedia). Instead: enumerate devices in renderer, this IPC just handles PTT and WS forwarding.
    - `IPC.VOICE_SET_PTT_KEY`: Call `changePTTKey(newKey)`.
    - `IPC.VOICE_GET_PTT_KEY`: Return current PTT key from settings (localStorage or electron-store).
    - `IPC.VOICE_CHECK_MIC_PERMISSION`: On macOS, call `systemPreferences.getMediaAccessStatus('microphone')`. On other platforms, return 'granted'. If not granted on macOS, call `systemPreferences.askForMediaAccess('microphone')`.
    - Register all handlers in the existing IPC registration pattern.

    **4. Update `client/src/main/ipc/channels.ts`:**
    Add voice IPC constants:
    ```typescript
    // Voice
    VOICE_JOIN: 'voice:join',
    VOICE_LEAVE: 'voice:leave',
    VOICE_SEND_SDP_OFFER: 'voice:send-sdp-offer',
    VOICE_SEND_SDP_ANSWER: 'voice:send-sdp-answer',
    VOICE_SEND_ICE_CANDIDATE: 'voice:send-ice-candidate',
    VOICE_SEND_STATE_UPDATE: 'voice:send-state-update',
    VOICE_SEND_SPEAKING: 'voice:send-speaking',
    VOICE_SET_PTT_KEY: 'voice:set-ptt-key',
    VOICE_GET_PTT_KEY: 'voice:get-ptt-key',
    VOICE_CHECK_MIC_PERMISSION: 'voice:check-mic-permission',
    VOICE_SET_MODE: 'voice:set-mode',  // 'vad' | 'ptt'

    // Push events
    PUSH_VOICE_EVENT: 'voice:event',
    PUSH_PTT_STATE: 'voice:ptt-state',
    ```

    **5. Update `client/src/preload/index.ts`:**
    Add `voice` namespace to the API:
    ```typescript
    voice: {
      join: (channelId: string) => ipcRenderer.invoke(IPC.VOICE_JOIN, channelId),
      leave: () => ipcRenderer.invoke(IPC.VOICE_LEAVE),
      sendSdpOffer: (targetUserId: string, sdp: string, channelId: string) =>
        ipcRenderer.invoke(IPC.VOICE_SEND_SDP_OFFER, targetUserId, sdp, channelId),
      sendSdpAnswer: (targetUserId: string, sdp: string, channelId: string) =>
        ipcRenderer.invoke(IPC.VOICE_SEND_SDP_ANSWER, targetUserId, sdp, channelId),
      sendIceCandidate: (targetUserId: string, candidateJson: string, channelId: string) =>
        ipcRenderer.invoke(IPC.VOICE_SEND_ICE_CANDIDATE, targetUserId, candidateJson, channelId),
      sendStateUpdate: (channelId: string, muted: boolean, deafened: boolean) =>
        ipcRenderer.invoke(IPC.VOICE_SEND_STATE_UPDATE, channelId, muted, deafened),
      sendSpeaking: (channelId: string, speaking: boolean) =>
        ipcRenderer.invoke(IPC.VOICE_SEND_SPEAKING, channelId, speaking),
      setPttKey: (key: number) => ipcRenderer.invoke(IPC.VOICE_SET_PTT_KEY, key),
      getPttKey: () => ipcRenderer.invoke(IPC.VOICE_GET_PTT_KEY),
      setMode: (mode: 'vad' | 'ptt') => ipcRenderer.invoke(IPC.VOICE_SET_MODE, mode),
      checkMicPermission: () => ipcRenderer.invoke(IPC.VOICE_CHECK_MIC_PERMISSION),
    },
    onVoiceEvent: (cb: (event: VoiceEvent) => void) => {
      const handler = (_: unknown, event: VoiceEvent) => cb(event)
      ipcRenderer.on(IPC.PUSH_VOICE_EVENT, handler)
      return () => ipcRenderer.removeListener(IPC.PUSH_VOICE_EVENT, handler)
    },
    onPttState: (cb: (active: boolean) => void) => {
      const handler = (_: unknown, active: boolean) => cb(active)
      ipcRenderer.on(IPC.PUSH_PTT_STATE, handler)
      return () => ipcRenderer.removeListener(IPC.PUSH_PTT_STATE, handler)
    },
    ```

    **6. Update `shared/ipc-bridge.d.ts`:**
    Add voice-related type definitions:
    ```typescript
    interface VoiceParticipant {
      userId: string
      displayName: string
      pubkey: string
      muted: boolean
      deafened: boolean
    }

    interface VoiceEvent {
      type: 'join_response' | 'participant_joined' | 'participant_left' | 'sdp_offer' | 'sdp_answer' | 'ice_candidate' | 'state_update' | 'speaking'
      data: unknown
    }

    interface VoiceJoinResponseData {
      participants: VoiceParticipant[]
      iceServers: Array<{ urls: string[]; username: string; credential: string }>
    }

    type VoiceMode = 'vad' | 'ptt'
    type ConnectionQuality = 'good' | 'degraded' | 'poor'

    interface VoiceQualityMetrics {
      rttMs: number
      packetLoss: number
      jitter: number
      quality: ConnectionQuality
    }
    ```
    Add `voice` to the `UnitedAPI` interface.

    **7. Create `client/src/renderer/src/stores/voice.ts`:**
    VoiceSlice following existing Zustand slice pattern:
    ```typescript
    interface VoiceSlice {
      // State
      voiceChannelId: string | null
      voiceParticipants: Map<string, VoiceParticipantState>
      localMuted: boolean
      localDeafened: boolean
      voiceMode: VoiceMode  // 'vad' | 'ptt'
      vadSensitivity: number  // 0-100 (0=sensitive, 100=aggressive)
      pttActive: boolean  // true when PTT key held
      connectionQuality: ConnectionQuality
      qualityMetrics: VoiceQualityMetrics | null
      inputDeviceId: string | null
      outputDeviceId: string | null
      outputVolume: number  // 0-100
      userVolumes: Record<string, number>  // userId -> 0-200

      // Actions
      joinVoiceChannel: (channelId: string) => Promise<void>
      leaveVoiceChannel: () => void
      toggleMute: () => void
      toggleDeafen: () => void
      setVoiceMode: (mode: VoiceMode) => void
      setVadSensitivity: (value: number) => void
      setPttActive: (active: boolean) => void
      setConnectionQuality: (quality: ConnectionQuality, metrics: VoiceQualityMetrics) => void
      setInputDevice: (deviceId: string) => void
      setOutputDevice: (deviceId: string) => void
      setOutputVolume: (volume: number) => void
      setUserVolume: (userId: string, volume: number) => void

      // Participant state updates (from WS events)
      addVoiceParticipant: (participant: VoiceParticipant) => void
      removeVoiceParticipant: (userId: string) => void
      updateParticipantState: (userId: string, muted: boolean, deafened: boolean) => void
      updateParticipantSpeaking: (userId: string, speaking: boolean) => void
    }
    ```
    - `VoiceParticipantState` extends `VoiceParticipant` with `speaking: boolean`, `quality: ConnectionQuality`.
    - `joinVoiceChannel`: Sends IPC join, does NOT manage WebRTC (that's VoiceManager's job). Sets voiceChannelId.
    - `leaveVoiceChannel`: Sends IPC leave, clears all voice state.
    - `toggleMute`: Flips localMuted. If deafened and unmuting, also undeafen (deafen implies mute per CONTEXT.md).
    - `toggleDeafen`: Flips localDeafened. If deafening, also mute.
    - Persist voice settings (mode, sensitivity, input/output device, userVolumes) to localStorage.
    - Load from localStorage on init.

    **8. Update `client/src/renderer/src/stores/index.ts`:**
    Import and spread VoiceSlice into the root store creator.

    **9. Create `client/src/renderer/src/hooks/useVoice.ts`:**
    - Manages VoiceManager lifecycle, WS event subscriptions, and cleanup.
    - `useVoice()` hook:
      - On mount: register WS voice event listener and PTT state listener via preload bridge.
      - When voiceChannelId changes to non-null: instantiate VoiceManager, AudioPipeline, SignalingClient. Wire WS events to VoiceManager handlers.
      - When voiceChannelId changes to null: dispose VoiceManager, AudioPipeline, SignalingClient.
      - Wire speaking detection to store updates.
      - Wire stats polling to store updates.
      - Return cleanup function that disposes everything on unmount.
    - This hook should be called once at a high level (e.g., Main.tsx) so voice persists across channel navigation.
  </action>
  <verify>
    cd /c/Users/matts/united/client && npx tsc --noEmit 2>&1 | tail -10
  </verify>
  <done>
    uiohook-napi installed. PTT module registers global hotkey with keydown/keyup. VoiceManager manages full-mesh WebRTC. AudioPipeline routes audio with per-user volume and VAD. VoiceSlice tracks all voice state in Zustand. useVoice hook manages lifecycle and WS event subscriptions. IPC bridge exposes voice namespace. TypeScript compiles without errors.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors (`npx tsc --noEmit`)
2. uiohook-napi installs and is importable from main process
3. VoiceManager creates RTCPeerConnection with proper ICE candidate queueing
4. AudioPipeline creates AudioContext, captures mic, routes remote streams
5. SignalingClient sends/receives all voice message types via IPC
6. VoiceSlice stores all voice state and exposes actions
7. Preload bridge exposes voice.* namespace
</verification>

<success_criteria>
- Client can send VoiceJoinRequest and receive VoiceJoinResponse with participant list and ICE servers
- VoiceManager creates peer connections to each existing participant with lexicographic offer/answer roles
- AudioPipeline captures local microphone with echo cancellation, noise suppression, and auto gain control
- Per-user volume control via GainNode (0-200%)
- Speaking detection via AnalyserNode RMS with configurable threshold
- Push-to-talk global hotkey works with keydown/keyup (not just keydown)
- Mute toggles MediaStreamTrack.enabled, deafen sets masterGain to 0
- Connection quality polled every 2 seconds from getStats
- All cleanup runs on voice channel leave (tracks stopped, connections closed, AudioContext closed)
</success_criteria>

<output>
After completion, create `.planning/phases/08-voice-channels/08-02-SUMMARY.md`
</output>
